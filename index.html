<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Stacking — Explained</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial; line-height: 1.5; color: #111; padding: 28px; max-width: 900px; margin: auto; }
    h1, h2 { color: #0b4; }
    h1 { margin-top: 6px; }
    .lead { color: #333; margin-bottom: 18px; }
    .card { border: 1px solid #e6e6e6; padding: 14px; border-radius: 8px; background: #fff; box-shadow: 0 1px 2px rgba(0,0,0,0.03); }
    .diagram { text-align: center; margin: 18px 0; }
    pre.definition { background: #fafafa; padding: 10px; border-radius: 6px; overflow: auto; }
    dl dt { font-weight: 700; margin-top: 10px; }
    ul { margin-top: 6px; }
    footer { color: #666; font-size: 14px; margin-top: 28px; }
  </style>
</head>
<body>
  <h1>Stacking — Explained</h1>
  

  <section class="card">
    <h2>Intro — what is stacking?</h2>
  <p>Stacking is an <a href="#ensemble">ensemble</a> learning technique that combines the predictions of multiple <a href="#model">models</a> with a final model called a <strong>meta-model</strong>. The meta-model learns how to best combine the models’ outputs so the overall system is more accurate and robust than any single model.</p>
  </section>

  <!-- examples removed from top; visuals will live directly under definitions below -->

  <section class="card" style="margin-top:12px">
    <h2>Quick explanation</h2>
    <p>At a high level, stacking starts by training several different base models on the same training set. Each base model makes its own predictions. A separate combining model — the meta-model — then learns how to use those base-model predictions to produce a single final prediction. In other words: first get a set of diverse predictions, then learn how to combine them well.</p>
  </section>

  <section class="card" style="margin-top:12px">
    <h2>Stacking design</h2>
    <p>The architecture typically has two layers:</p>
    <ol>
      <li><strong>Base models (Level 0)</strong>: multiple individual learners trained on the original dataset. Examples: decision trees, logistic regression, random forests, k-nearest neighbors, and others.</li>
      <li><strong>Meta-model (Level 1)</strong>: a single model trained on the base models’ predictions (often called meta-features). Common choices for the meta-model are simple learners like linear or logistic regression, but any learner that suits the problem can be used.</li>
    </ol>

    <div class="diagram">
      <h3>Figure 1 — Simple stacking diagram</h3>
      <!-- Simple SVG diagram of stacking -->
      <svg width="600" height="230" viewBox="0 0 600 230" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Stacking diagram">
        <!-- training dataset -->
        <rect x="20" y="18" width="170" height="40" rx="6" fill="#f3f8ff" stroke="#cfe0ff" />
        <text x="30" y="44" font-family="sans-serif" font-size="13" fill="#033">Training dataset</text>

        <!-- base models -->
        <rect x="220" y="6" width="110" height="40" rx="6" fill="#fff6f3" stroke="#ffd7c2"/>
        <text x="231" y="30" font-family="sans-serif" font-size="12">Base model A</text>
        <rect x="220" y="66" width="110" height="40" rx="6" fill="#f6fff4" stroke="#c7f3cf"/>
        <text x="231" y="90" font-family="sans-serif" font-size="12">Base model B</text>
        <rect x="220" y="126" width="110" height="40" rx="6" fill="#fff9f6" stroke="#ffe6c9"/>
        <text x="231" y="150" font-family="sans-serif" font-size="12">Base model C</text>

        <!-- arrows from dataset to base models -->
        <line x1="190" y1="38" x2="220" y2="26" stroke="#888" stroke-width="1.6" marker-end="url(#arrow)"/>
        <line x1="190" y1="38" x2="220" y2="86" stroke="#888" stroke-width="1.6" marker-end="url(#arrow)"/>
        <line x1="190" y1="38" x2="220" y2="146" stroke="#888" stroke-width="1.6" marker-end="url(#arrow)"/>

        <!-- meta model -->
        <rect x="420" y="78" width="130" height="48" rx="8" fill="#eefaff" stroke="#c6e9ff"/>
        <text x="435" y="105" font-family="sans-serif" font-size="13">Meta-model</text>

        <!-- arrows from base models to meta-model -->
        <line x1="330" y1="26" x2="420" y2="102" stroke="#444" stroke-width="1.4" marker-end="url(#arrow)"/>
        <line x1="330" y1="86" x2="420" y2="102" stroke="#444" stroke-width="1.4" marker-end="url(#arrow)"/>
        <line x1="330" y1="146" x2="420" y2="102" stroke="#444" stroke-width="1.4" marker-end="url(#arrow)"/>

        <!-- final prediction -->
        <rect x="560" y="78" width="30" height="48" rx="6" fill="#fff" stroke="#ddd"/>
        <text x="568" y="105" font-family="sans-serif" font-size="11">Output</text>
        <line x1="550" y1="102" x2="560" y2="102" stroke="#444" stroke-width="1.2" marker-end="url(#arrow)"/>

        <defs>
          <marker id="arrow" markerWidth="6" markerHeight="6" refX="5" refY="3" orient="auto">
            <path d="M0,0 L6,3 L0,6" fill="#444" />
          </marker>
        </defs>
      </svg>
  <p style="font-size:13px;color:#555;">Note: the diagram shows the high-level flow from base models to the meta-model. See the sections below for training and deployment details.</p>
    </div>
  </section>

  


  <section class="card" style="margin-top:12px">
    <h2>How stacking works</h2>
    <p>Briefly, stacking proceeds in a few clear stages:</p>
    <ol>
      <li>Train several base models on the available training data; each model makes its own predictions.</li>
      <li>Use those base-model predictions as inputs to a meta-model that learns how to combine them into a final prediction.</li>
      <li>For deployment, a common approach is to train each base model on the full training set and let the meta-model produce the final output.</li>
    </ol>

    <h2 style="margin-top:16px">Definitions and examples</h2>
    <dl>
  <dt id="ensemble">Ensemble</dt>
      <dd>
        A method that combines the outputs from multiple runs of the same model (the same algorithm) to produce a single prediction that is usually more reliable than any single run. Running the same algorithm multiple times with different random seeds, data samples, or hyperparameters produces slightly different models; combining their outputs (by averaging or voting) reduces random errors and variance. Example: Random Forest trains many decision trees on different random samples and averages their predictions.
      </dd>
        <div style="margin-top:8px; text-align:center;">
          <!-- Ensemble: two-panel showing model1 + model2 => averaged model3 -->
          <svg width="520" height="120" viewBox="0 0 520 120" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
            <rect width="520" height="120" fill="#fff" stroke="#f6f6f6"/>
            <!-- left panel: model A and model B overlays -->
            <g transform="translate(10,10)">
              <rect x="0" y="0" width="180" height="90" fill="#fff" stroke="#eee"/>
              <!-- data points -->
              <g fill="#c44">
                <circle cx="18" cy="72" r="2.8" />
                <circle cx="48" cy="58" r="2.8" />
                <circle cx="78" cy="46" r="2.8" />
                <circle cx="108" cy="38" r="2.8" />
                <circle cx="138" cy="30" r="2.8" />
                <circle cx="168" cy="24" r="2.8" />
              </g>
              <!-- model 1 line -->
              <path d="M12,76 C40,60 70,48 100,38 C130,30 160,26 172,24" stroke="#4a90e2" stroke-width="2.5" fill="none"/>
              <!-- model 2 line -->
              <path d="M12,74 C40,62 70,50 100,44 C130,36 160,28 172,26" stroke="#7ed321" stroke-width="2.5" fill="none"/>
              <text x="90" y="106" font-size="11" text-anchor="middle" fill="#333">Model A + Model B (overlay)</text>
            </g>

            <!-- arrow to averaged panel -->
            <g transform="translate(200,44)">
              <path d="M0,4 L40,4" stroke="#666" stroke-width="2" marker-end="url(#arrow)"/>
              <text x="20" y="-6" font-size="11" text-anchor="middle" fill="#666">average →</text>
            </g>

            <!-- right panel: averaged result -->
            <g transform="translate(260,10)">
              <rect x="0" y="0" width="180" height="90" fill="#fff" stroke="#eee"/>
              <!-- same data points -->
              <g fill="#c44">
                <circle cx="18" cy="72" r="2.8" />
                <circle cx="48" cy="58" r="2.8" />
                <circle cx="78" cy="46" r="2.8" />
                <circle cx="108" cy="38" r="2.8" />
                <circle cx="138" cy="30" r="2.8" />
                <circle cx="168" cy="24" r="2.8" />
              </g>
              <!-- averaged line (simple mean of the two) -->
              <path d="M12,75 C40,61 70,49 100,41 C130,33 160,27 172,25" stroke="#333" stroke-width="2.8" fill="none"/>
              <text x="90" y="106" font-size="11" text-anchor="middle" fill="#333">Averaged result (Model C)</text>
            </g>

            <defs>
              <marker id="arrow" markerWidth="6" markerHeight="6" refX="5" refY="3" orient="auto">
                <path d="M0,0 L6,3 L0,6" fill="#666" />
              </marker>
            </defs>
          </svg>
        </div>

  <dt id="model">Model</dt>
      <dd>A model is a function learned from data that maps inputs to predictions. Typically you split data into training and test sets (for example 80/20). You use the training data to fit a model (e.g., a decision tree, a linear equation, or a neural network). After training, the model takes new inputs and produces predicted outputs.</dd>
      <div style="margin-top:8px; text-align:center;">
          <svg width="320" height="90" viewBox="0 0 320 90" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
            <rect width="320" height="90" fill="#fff" stroke="#f0f0f0"/>
            <g transform="translate(12,10)">
              <!-- scatter plot -->
              <g>
                <rect x="0" y="0" width="120" height="60" fill="#fff" stroke="#eee"/>
                <circle cx="12" cy="48" r="2.5" fill="#c44"/>
                <circle cx="36" cy="38" r="2.5" fill="#c44"/>
                <circle cx="60" cy="28" r="2.5" fill="#c44"/>
                <circle cx="84" cy="22" r="2.5" fill="#c44"/>
                <polyline points="6,50 30,40 54,30 78,24" fill="none" stroke="#257" stroke-width="2"/>
              </g>
              <!-- arrow -->
              <g transform="translate(136,22)">
                <polygon points="0,6 12,0 12,12" fill="#888"/>
              </g>
              <!-- model box -->
              <g transform="translate(156,8)">
                <rect x="0" y="0" width="120" height="44" rx="6" fill="#eefaff" stroke="#c6e9ff"/>
                <text x="60" y="26" font-size="12" fill="#033" text-anchor="middle">Model</text>
              </g>
            </g>
          </svg>
        </div>

      <dt>Meta-model</dt>
      <dd>The model that learns to combine other models' outputs into the final prediction in a stacking setup.</dd>
      <!-- Visual omitted here; see Figure 1 above for the stacking diagram -->

      

      <dt>Overfitting</dt>
      <dd>When a model fits the training data too closely — for example a jagged curve that passes through every point instead of a smooth curve that captures the general trend. An overfitted model performs well on training data but poorly on new data because it learned noise instead of the underlying pattern.</dd>
      <div style="margin-top:8px; text-align:center;">
        <div style="margin-top:8px; text-align:center; display:flex; gap:18px; justify-content:center;">
          <!-- Restored original two overfitting graphs side-by-side -->
          <div style="text-align:center;">
            <svg width="220" height="120" viewBox="0 0 220 120" xmlns="http://www.w3.org/2000/svg">
              <rect width="220" height="120" fill="#fff" stroke="#eee"/>
              <g fill="#c44">
                <circle cx="25" cy="90" r="3" />
                <circle cx="55" cy="75" r="3" />
                <circle cx="85" cy="60" r="3" />
                <circle cx="115" cy="50" r="3" />
                <circle cx="145" cy="40" r="3" />
                <circle cx="175" cy="30" r="3" />
              </g>
              <path d="M20,95 C50,80 90,60 120,48 C150,36 190,28 200,25" stroke="#257" stroke-width="2.5" fill="none" />
              <text x="110" y="112" font-size="11" text-anchor="middle" fill="#333">Normal (smooth)</text>
            </svg>
          </div>

          <div style="text-align:center;">
            <svg width="220" height="120" viewBox="0 0 220 120" xmlns="http://www.w3.org/2000/svg">
              <rect width="220" height="120" fill="#fff" stroke="#eee"/>
              <g fill="#c44">
                <circle cx="25" cy="90" r="3" />
                <circle cx="55" cy="75" r="3" />
                <circle cx="85" cy="60" r="3" />
                <circle cx="115" cy="50" r="3" />
                <circle cx="145" cy="40" r="3" />
                <circle cx="175" cy="30" r="3" />
              </g>
              <path d="M20,95 C30,92 40,70 55,76 C70,82 82,68 92,60 C102,52 110,62 118,46 C126,30 140,44 150,36 C160,28 180,26 200,25" stroke="#b02" stroke-width="2.5" fill="none" />
              <text x="110" y="112" font-size="11" text-anchor="middle" fill="#333">Overfit (jagged)</text>
            </svg>
          </div>
        </div>

      <dt>Bagging</dt>
      <dd>Bootstrap aggregating: training the same algorithm multiple times on different random samples and averaging the results (e.g., Random Forest).</dd>

      <dt>Boosting</dt>
      <dd>Sequentially building models where each new model focuses on correcting errors from the previous ones (e.g., AdaBoost, Gradient Boosting).</dd>

      <dt>Voting / Averaging</dt>
      <dd>Simple ensemble strategies: voting picks the majority class among classifiers; averaging combines numeric predictions or probabilities.</dd>
    </dl>
  </section>

  <section class="card" style="margin-top:12px">
    <h2>Advantages of stacking</h2>
    <ul>
      <li><strong>Better performance</strong>: combining models often increases accuracy and reliability.</li>
      <li><strong>Combines different strengths</strong>: you can mix diverse model types so their different biases complement each other.</li>
  <li><strong>Reduces overfitting</strong>: combining multiple models can help balance errors and reduce overfitting compared to relying on a single model.</li>
      <li><strong>Learns from mistakes</strong>: the meta-model can learn where certain base models systematically err and correct them.</li>
      <li><strong>Customizable</strong>: you can choose base and meta-models to suit the dataset and problem.</li>
    </ul>
  </section>

  <section class="card" style="margin-top:12px">
    <h2>Limitations of stacking</h2>
    <ul>
      <li><strong>Implementation complexity</strong>: stacking requires more careful setup than single-model approaches and some other ensemble methods.</li>
      <li><strong>Slower training</strong>: multiple base models plus a meta-model increases compute and training time.</li>
      <li><strong>Harder to interpret</strong>: the final prediction comes from several models, which can make explanations more difficult.</li>
      <li><strong>Risk of overfitting</strong>: if the meta-model is too flexible or if data leakage occurs, stacking can overfit.</li>
      <li><strong>Needs more data</strong>: it benefits from more data, especially because both base and meta-models need training examples.</li>
    </ul>
  </section>

  <section class="card" style="margin-top:12px">
    <h2>Common model types (examples)</h2>
    <p>Here are common model families you might use as base models in a stacking setup, and a brief note on what they look for in data:</p>
    <ul>
      <li><strong>Linear models</strong> (e.g., linear regression, logistic regression): assume a linear relationship between inputs and output; simple and fast, good baseline.</li>
      <li><strong>Decision trees</strong>: partition the feature space with axis-aligned splits; capture non-linear interactions and are easy to interpret at a local level.</li>
      <li><strong>Ensembles of trees</strong> (e.g., Random Forest, Gradient Boosting): combine many trees for strong predictive performance; often a strong choice for tabular data.</li>
      <li><strong>k-Nearest Neighbors (k-NN)</strong>: predict based on nearby training examples; non-parametric and simple, but can be slow at inference for large datasets.</li>
      <li><strong>Support Vector Machines (SVM)</strong>: find a margin-maximizing boundary; effective in high-dimensional spaces and with kernels for non-linear separation.</li>
      <li><strong>Neural networks</strong>: flexible function approximators that learn layered representations; can model complex patterns but need more data and tuning.</li>
    </ul>
  </section>

  <section class="card" style="margin-top:12px">
    <h2>Resources</h2>
    <p>These pages were helpful while preparing this summary:</p>
    <ul>
      <li><a href="https://www.geeksforgeeks.org/machine-learning/stacking-in-machine-learning/" target="_blank" rel="noopener">GeeksforGeeks — Stacking in Machine Learning</a></li>
      <li><a href="https://medium.com/@brijesh_soni/stacking-to-improve-model-performance-a-comprehensive-guide-on-ensemble-learning-in-python-9ed53c93ce28" target="_blank" rel="noopener">Medium — Stacking to improve model performance (guide)</a></li>
    </ul>
  </section>

  
</body>
</html>
